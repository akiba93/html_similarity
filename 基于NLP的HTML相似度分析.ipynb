{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d1b1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model link: https://huggingface.co/alecsharpie/codegen_350m_html\n",
    "# tokenizer max length 2048\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_sentence_embedding(text,tokenizer,model):\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "    #model = AutoModelForCausalLM.from_pretrained(\"alecsharpie/codegen_350m_html\")\n",
    "\n",
    "    t_input = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    if len(t_input['input_ids'][0]) > 2048:\n",
    "        # if sentence is longer than max length\n",
    "        input_id_chunks = list(t_input['input_ids'][0].split(2048))\n",
    "        mask_chunks = list(t_input['attention_mask'][0].split(2048))\n",
    "        for i in range(len(input_id_chunks)):\n",
    "            # get required padding length\n",
    "            pad_len = 2048 - input_id_chunks[i].shape[0]\n",
    "            # check if tensor length satisfies required chunk size\n",
    "            if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "                input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([tokenizer.pad_token_id] * pad_len)])\n",
    "                mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
    "    \n",
    "        # new input    \n",
    "        input_ids = torch.stack(input_id_chunks)\n",
    "        attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "        input_dict = {\n",
    "            'input_ids': input_ids.long(),\n",
    "            'attention_mask': attention_mask.int()\n",
    "        }\n",
    "    else:\n",
    "        input_dict = t_input\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**input_dict, output_hidden_states=True).hidden_states[-1]\n",
    "    \n",
    "    text_embedding = torch.mean(last_hidden_state,dim = 1).reshape(-1)\n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "920c8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding_similarity(text1_embedding,text2_embedding):\n",
    "    text1_embedding_len = len(text1_embedding)\n",
    "    text2_embedding_len = len(text2_embedding)\n",
    "\n",
    "    max_len = max(text1_embedding_len,text2_embedding_len)\n",
    "\n",
    "    if text1_embedding_len < max_len:\n",
    "        pad_len = max_len - text1_embedding_len\n",
    "        text1_embedding = torch.cat([text1_embedding, torch.Tensor([0] * pad_len)])\n",
    "    elif text2_embedding_len < max_len:\n",
    "        pad_len = max_len - text2_embedding_len\n",
    "        text2_embedding = torch.cat([text2_embedding, torch.Tensor([0] * pad_len)])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    embedding_simi = torch.cosine_similarity(text1_embedding,text2_embedding, dim = 0).item()\n",
    "    \n",
    "    return embedding_simi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e76fd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url1,url2):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs, NavigableString\n",
    "    from urllib.parse import urljoin\n",
    "\n",
    "    # initialize a session & set User-Agent as a regular browser\n",
    "    session = requests.Session()\n",
    "    session.headers[\"User-Agent\"] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "\n",
    "    # get the HTML content\n",
    "    html1 = session.get(url1).content\n",
    "    html2 = session.get(url2).content\n",
    "\n",
    "    # parse HTML using beautiful soup\n",
    "    soup1 = bs(html1, \"html.parser\")\n",
    "    soup2 = bs(html2, \"html.parser\")\n",
    "    \n",
    "    return soup1,soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8963a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_similarity(soup1,soup2,weight):\n",
    "    from bs4 import BeautifulSoup as bs, Tag, NavigableString\n",
    "    \n",
    "    #head similarity\n",
    "    head_simi = 0\n",
    "    \n",
    "    html1_head_text_list = [str(i) for i in soup1.head.find_all()]\n",
    "    html2_head_text_list = [str(i) for i in soup2.head.find_all()]\n",
    "    head_min_len = min(len(html1_head_text_list),len(html2_head_text_list))\n",
    "    head_max_len = max(len(html1_head_text_list),len(html2_head_text_list))\n",
    "    \n",
    "    for i in range(head_min_len):\n",
    "        text1 = html1_head_text_list[i]\n",
    "        text2 = html2_head_text_list[i]\n",
    "        \n",
    "        text1_embedding = get_sentence_embedding(text1,tokenizer,model)\n",
    "        text2_embedding = get_sentence_embedding(text2,tokenizer,model)\n",
    "        \n",
    "        simi = text_embedding_similarity(text1_embedding,text2_embedding)\n",
    "        head_simi += simi\n",
    "    \n",
    "    head_simi = head_simi / head_min_len\n",
    "    \n",
    "    #body similarity\n",
    "    body_simi = 0\n",
    "    \n",
    "    html1_body_text_list = [str(i) for i in soup1.body.find_all()]\n",
    "    html2_body_text_list = [str(i) for i in soup2.body.find_all()]\n",
    "    body_min_len = min(len(html1_body_text_list),len(html2_body_text_list))\n",
    "    body_max_len = max(len(html1_body_text_list),len(html2_body_text_list))\n",
    "    \n",
    "    for i in range(body_min_len):\n",
    "        text1 = html1_body_text_list[i]\n",
    "        text2 = html2_body_text_list[i]\n",
    "        \n",
    "        text1_embedding = get_sentence_embedding(text1,tokenizer,model)\n",
    "        text2_embedding = get_sentence_embedding(text2,tokenizer,model)\n",
    "        \n",
    "        simi = text_embedding_similarity(text1_embedding,text2_embedding)\n",
    "        body_simi += simi\n",
    "    \n",
    "    body_simi = body_simi / body_max_len\n",
    "    \n",
    "    #html_similarity\n",
    "    html_similarity = weight * head_simi + (1 - weight) * body_simi\n",
    "    \n",
    "    return round(html_similarity,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e821944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html1 = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie,I have a dream</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b96def2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html2 = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie,I have a dream</a>,\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "230c819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"alecsharpie/codegen_350m_html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a4291be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71516\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "soup1 = bs(html1, \"html.parser\")\n",
    "soup2 = bs(html2, \"html.parser\")\n",
    "print(html_similarity(soup1,soup2,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "71da3a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (51042 > 2048). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://www.baidu.com\"\n",
    "url2 = \"https://www.baidu.com\"\n",
    "soup1,soup2 = get_html(url1,url2)\n",
    "html_similarity(soup1,soup2,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fd90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
